%& -shell-escape
\title{Linear Regression}
\author{Ahnaf An Nafee}
\date{February 2021}
\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{comment}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}

\graphicspath{ {./images/} }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}


\begin{document}

\maketitle

\section{Theory}

\begin{enumerate}
	\item LR Supervised Training Dataset:


\begin{enumerate}
	
	\item Coefficients for Closed-form Linear Regression:\\

		$
		X=
		\begin{bmatrix}
			1 & -2\\
			1 & -5\\	
			1 & -3\\
			1 & 0\\
			1 & -8\\
			1 & -2\\
			1 & 1\\
			1 & 5\\
			1 & -1\\
			1 & 6\\
		\end{bmatrix},
	Y=
		\begin{bmatrix}
			1\\
			-4\\	
			1\\
			3\\
			11\\
			5\\
			0\\
			-1\\
			-3\\
			1\\
		\end{bmatrix}
		$



	\begin{equation}
		\begin{split}
			w = \frac{1}{N} (X^{T}X)^{-1}X^{T}Y
		\end{split}
	\end{equation}

	\begin{equation}
	\begin{split}
		w = \frac{1}{10} (\begin{bmatrix}
			1 & -2\\
			1 & -5\\	
			1 & -3\\
			1 & 0\\
			1 & -8\\
			1 & -2\\
			1 & 1\\
			1 & 5\\
			1 & -1\\
			1 & 6\\
		\end{bmatrix}^{T}\begin{bmatrix}
			1 & -2\\
			1 & -5\\	
			1 & -3\\
			1 & 0\\
			1 & -8\\
			1 & -2\\
			1 & 1\\
			1 & 5\\
			1 & -1\\
			1 & 6\\
		\end{bmatrix})^{-1}\begin{bmatrix}
			1 & -2\\
			1 & -5\\	
			1 & -3\\
			1 & 0\\
			1 & -8\\
			1 & -2\\
			1 & 1\\
			1 & 5\\
			1 & -1\\
			1 & 6\\
		\end{bmatrix}^{T}\begin{bmatrix}
			1\\
			-4\\	
			1\\
			3\\
			11\\
			5\\
			0\\
			-1\\
			-3\\
			1\\
			\end{bmatrix}
		\end{split}
	\end{equation}

\begin{equation}
	\begin{split}
		w = \frac{1}{10} (\begin{bmatrix}
			1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
			-2 & -5 & -3 & 0 & -8 & -2 & 1 & 5 & -1 & 6\\
		\end{bmatrix}\begin{bmatrix}
			1 & -2\\
			1 & -5\\	
			1 & -3\\
			1 & 0\\
			1 & -8\\
			1 & -2\\
			1 & 1\\
			1 & 5\\
			1 & -1\\
			1 & 6\\
		\end{bmatrix})^{-1}\begin{bmatrix}
		1 & -2\\
		1 & -5\\	
		1 & -3\\
		1 & 0\\
		1 & -8\\
		1 & -2\\
		1 & 1\\
		1 & 5\\
		1 & -1\\
		1 & 6\\
	\end{bmatrix}^{T}\begin{bmatrix}
			1\\
			-4\\	
			1\\
			3\\
			11\\
			5\\
			0\\
			-1\\
			-3\\
			1\\
		\end{bmatrix}
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		w = \frac{1}{10} (\begin{bmatrix}
			10 & -9\\
			-9 & 169\\
		\end{bmatrix})^{-1}\begin{bmatrix}
		1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
		-2 & -5 & -3 & 0 & -8 & -2 & 1 & 5 & -1 & 6\\
	\end{bmatrix}\begin{bmatrix}
			1\\
			-4\\	
			1\\
			3\\
			11\\
			5\\
			0\\
			-1\\
			-3\\
			1\\
		\end{bmatrix}
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		w = \frac{1}{10} (\begin{bmatrix}
			0.10503418 & 0.00559354\\
			0.00559354 & 0.00621504\\
		\end{bmatrix})\begin{bmatrix}
			1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
			-2 & -5 & -3 & 0 & -8 & -2 & 1 & 5 & -1 & 6\\
		\end{bmatrix}\begin{bmatrix}
			1\\
			-4\\	
			1\\
			3\\
			11\\
			5\\
			0\\
			-1\\
			-3\\
			1\\
		\end{bmatrix}
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		w = \begin{bmatrix}
			0.010503418 & 0.000559354\\
			0.000559354 & 0.000621504\\
		\end{bmatrix}\begin{bmatrix}
			14\\
			-79\\
		\end{bmatrix}
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		w = \begin{bmatrix}
			0.102858919\\
			-0.041267868\\
		\end{bmatrix}
	\end{split}
\end{equation}



		\begin{comment}
	\newpage
	\end{comment}
	
	\item Predictions, Y , for the Training Data:\\
	
	$
		Y_{pred}=
	\begin{bmatrix}
		0.18539466\\
		0.30919826\\	
		0.22666252\\
		0.10285892\\
		0.43300186\\
		0.18539466\\
		0.06159105\\
		-0.10348042\\
		0.14412679\\
		-0.14474829\\
	\end{bmatrix}
	$


	\item RMSE:\\
	

	
	\textbf{RMSE}: $-0.7071067811865475$

		
	
	\end{enumerate}
	
	\begin{comment}
	\newpage
		\end{comment}


	\item Function Weights:\\
	
	\begin{enumerate}
	
	\item Partial Gradients:\\
	
	$J=(x_1 w_1 -5x_2 w_2-2)^2$ = $(x_1 w_1 -5x_2 w_2-2)$$(x_1 w_1 -5x_2 w_2-2)$\\
	$J=x_1^{2}w_1^{2} - 5x_{1}x_{2}w_{1}w_{2}-2x_{1}w_{1}-5x_{1}x_{2}w_{1}w_{2} + 25x_2^{2}w_2^{2} + 10x_2-2x_{1}w_{1} +10x_{2}w_{2} + 4$\\
	$J=x_1^{2}w_1^{2} - 10x_{1}x_{2}w_{1}w_{2} -4x_{1}w_{1} + 25x_2^{2}w_2^{2} + 10x_2 +10x_{2}w_{2} + 4$\\
	
	
	\begin{equation}
		\begin{split}
			\frac{\partial J}{\partial w_1} = 2x_1^{2}w_1 - 10x_{1}x_{2}w_{2} - 4x_{1}
		\end{split}
	\end{equation}

	\begin{equation}
		\begin{split}
			\frac{\partial J}{\partial w_2} = -10x_{1}x_{2}w_{1} + 50x_2^{2}w_2 + 10 + 10x_2
		\end{split}
	\end{equation}
	
	\item Value of the Partial Gradients:\\
	
	\begin{tabular}{ c c}
		$w_1 = 0$ & $x_1 = 1$\\ 
		$w_2 = 0$ & $x_2 = 1$ 
	\end{tabular}\\
\\
	
	$\frac{\partial J}{\partial w_1} = 2 \times 0^2 \times 1 - 10 \times 1 \times 1 \times 0 - 4 \times 1 = -4$\\
	$\frac{\partial J}{\partial w_2} = - 10 \times 1 \times 1 \times 0 + 50 \times 1^2 \times 0 + 10 + 10 \times 1 = 20$\\
	

	\end{enumerate}

	
	\begin{comment}

	\end{comment}
	

\end{enumerate}

\newpage

\section{Gradient Descent}

	\begin{enumerate}
		
		\item Plot of epoch vs $J$:\\
		
		
		
		\item 3D plot of $w_1$ vs $w_2$, vs $J$:\\
		
		
		
		\item Final learned values of $w_1$, $w_2$, and $J$, and the number of epochs required to get there:\\
		
		
	\end{enumerate}
	


\section{Closed Form Linear Regression}




\end{document}
